{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avXMPuUTam4_"
   },
   "source": [
    "# Описание проекта\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.\n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "Постройте модель со значением метрики качества **F1 не меньше 0.75**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDQClO57am5A"
   },
   "source": [
    "## Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели.\n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять BERT необязательно, но вы можете попробовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_cjcAQiam5B"
   },
   "source": [
    "## Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`.\n",
    "\n",
    "Столбец `text` в нём содержит текст комментария, а `toxic` — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsisghEl463Y"
   },
   "source": [
    "Проверим наличие GPU и установим нужные зависимости."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YwPO7ONN4h-A",
    "outputId": "0b765644-eeab-4121-b3e4-5855d618178d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "5n6i8gKJcWPr",
    "outputId": "8968fc38-5e92-4f06-a61b-9cf842e36e9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xouf_E85am5C"
   },
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rfwXVqKqam5D"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import transformers as ppb\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from tqdm import notebook\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "s-9YnB1kam5I"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "dKA53bVNam5L",
    "outputId": "595a122a-98e2-49e6-917c-dd669cfd0ae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "OHPaCd69am5P",
    "outputId": "6be9c9c2-a731-4ac8-baff-6bcb7a022140"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50446</th>\n",
       "      <td>, and redirect the other names to it</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81571</th>\n",
       "      <td>SineBot1\\nPlease read the above comments.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25983</th>\n",
       "      <td>Thank you for your very good answer.  I just r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39022</th>\n",
       "      <td>\"\\n I think we need to ask who is likely to be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49431</th>\n",
       "      <td>Orangemonster2k1|SVRTVDude]] (VT) 23:26, 30 April</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79072</th>\n",
       "      <td>\" May 2009 (UTC)\\n\\n^^THANK GOD THERES SOMEBOD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24907</th>\n",
       "      <td>Incan writing? \\n\\nDid the Incas have any writ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108333</th>\n",
       "      <td>\"\\nGive it up, you're \"\"pissing in the wind\"\" ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50023</th>\n",
       "      <td>She did not die though, it's ok dont worry</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157296</th>\n",
       "      <td>Fixed reference in the article.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "50446                , and redirect the other names to it      0\n",
       "81571           SineBot1\\nPlease read the above comments.      0\n",
       "25983   Thank you for your very good answer.  I just r...      0\n",
       "39022   \"\\n I think we need to ask who is likely to be...      0\n",
       "49431   Orangemonster2k1|SVRTVDude]] (VT) 23:26, 30 April      0\n",
       "79072   \" May 2009 (UTC)\\n\\n^^THANK GOD THERES SOMEBOD...      0\n",
       "24907   Incan writing? \\n\\nDid the Incas have any writ...      0\n",
       "108333  \"\\nGive it up, you're \"\"pissing in the wind\"\" ...      0\n",
       "50023          She did not die though, it's ok dont worry      0\n",
       "157296                    Fixed reference in the article.      0"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "i25WSlnXam5S",
    "outputId": "5281719e-812f-4d55-eb66-f7823a53d10c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPB8chdIam5V"
   },
   "source": [
    "У нас датасет с 159571 неочищенными комментариями на английском языке и с большим дисбалансом классов. Пропусков в датасете нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmC4QCjbp5Gw"
   },
   "source": [
    "Выделим признак и целевой признак, а так же разобьём на тренировачную и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0yqHz4zFWYb2"
   },
   "outputs": [],
   "source": [
    "text = data['text']\n",
    "labels = data['toxic']\n",
    "\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(text, labels, test_size=.2, stratify=labels, random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdVv5PxIqJz_"
   },
   "source": [
    "Посмотрим на количество данных в каждой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "hzUDMoCyW89a",
    "outputId": "a0008163-8373-4b6b-c449-accff59fd17d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной выборки: 127656\n",
      "Размер тестовой выборки: 31915\n",
      "\n",
      "Классы: [0 1]\n"
     ]
    }
   ],
   "source": [
    "print('Размер тренировочной выборки: {}'.format(len(train_text)))\n",
    "print('Размер тестовой выборки: {}'.format(len(test_text)))\n",
    "print()\n",
    "print('Классы:', np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYjjyduc_7pd"
   },
   "source": [
    "Ниже вы воспользуемся более простым способом для предсказывания токсичности комментария:\n",
    "1. очистим тексты от лишних символов\n",
    "2. лемматизируем наши тексты\n",
    "3. уберем стоп слова\n",
    "4. вычислим TF-IDF\n",
    "5. создадим и натренируем модель логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0IAbVfLJUYDw"
   },
   "outputs": [],
   "source": [
    "def clean_text(value):\n",
    "    value = re.sub(r\"http\\S+\", \"\", value)\n",
    "    value = re.sub(r\"http\", \"\", value)\n",
    "    value = re.sub(\"([<>-])|[[:punct:]]\", \"\\\\1\", value)\n",
    "    value = re.sub(\"\\n\", \" \", value)\n",
    "    value = re.sub(\"(\\\\S+\\\\d+|\\\\d+)\\\\S+\", \"\", value)\n",
    "    value = re.sub(\"([<>-])|[[:punct:]]\", \"\\\\1\", value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "yse80Fs_bg4G",
    "outputId": "7c8d332c-f47b-46b4-e94b-5be9145fd3ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KeXcJA9MbeSU"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fAg0fdTsYBrc"
   },
   "outputs": [],
   "source": [
    "train_corpus = train_text.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "d5EqmmLWq4Pk",
    "outputId": "e31855f6-5d71-4261-f781-8716590e3288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135751    royal urban legends you want one link and he w...\n",
       "116496                 and take almost everything literally\n",
       "134498    schmuckythecat move the draft into articlespac...\n",
       "39542     yup once the article be there add it to the li...\n",
       "119967    also also your version be now slightly ambiguo...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "_x-N-A-uAkuZ",
    "outputId": "7530b0b8-2fa3-4735-c861-31f8d7c7f6df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "train_tf_idf = tf_idf_vectorizer.fit_transform(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9cMHLE_Xo_IH",
    "outputId": "892341a7-cb7f-4410-891c-640efc0e7769"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127656, 134746)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "xZywZHtCpdg1",
    "outputId": "c787f7a5-52fc-434b-d7b8-38cfc7f7e893"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=10000, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=-2,\n",
       "             param_grid={'C': [100, 10, 1.0, 0.1, 0.01], 'penalty': ['l2'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "\n",
    "logistic_regression_model = GridSearchCV(LogisticRegression(max_iter=10000), param_grid=grid,\n",
    "                   cv=5, n_jobs = -2)\n",
    "\n",
    "logistic_regression_model.fit(train_tf_idf, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "7pVr3zFOCUoO",
    "outputId": "cd465d46-cea2-470a-dc33-e460752d5e04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "MJNF8JUoqGlO",
    "outputId": "faaf95e5-9c8c-46fe-d0ac-89d216f727e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24806     because you know everything hi sandstein if yo...\n",
       "1956      consider the legal issue surround roms it woul...\n",
       "143203    i be demand to speak to someone of the highest...\n",
       "125561    re read the article i link to because that sam...\n",
       "20953                             o dieeeeeeeeeeeeeeeeeeeee\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus = test_text.apply(preprocess_text)\n",
    "test_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "nm99DS-LrXMm",
    "outputId": "6159410f-3a7b-4d1d-8ad2-543d557ef191"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 134746)"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tf_idf = tf_idf_vectorizer.transform(test_corpus)\n",
    "\n",
    "test_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6rA70baZp_vK"
   },
   "outputs": [],
   "source": [
    "logistic_regression_predictions = logistic_regression_model.predict(test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xYThbIr4rmtl",
    "outputId": "e35c797c-494a-4486-f945-8c58998d46b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression F1-score: 0.7674418604651163\n"
     ]
    }
   ],
   "source": [
    "print('LogisticRegression F1-score: {}'.format(f1_score(test_labels, logistic_regression_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXbarpO9Edua"
   },
   "source": [
    "Нам удалось достичь нужного результата на логистической регрессии f1-score 0.76, обучение и предсказания происходят быстро."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-1nTyuYam5V"
   },
   "source": [
    "Попробуем более сложную модель, а именно Bert. Для эмбединга и классификации воспользуемся моделью DistilBertForSequenceClassification, загрузим \n",
    "претренированный токенайзер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eSISmqwpbs2d"
   },
   "outputs": [],
   "source": [
    "tokenizer_class, pretrained_weights = (ppb.DistilBertTokenizer, 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "cuy5D2_yam5Z"
   },
   "outputs": [],
   "source": [
    "tokenizer = ppb.DistilBertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp2ZrPPeq13k"
   },
   "source": [
    "Возьмём сниппет функции помогающей отслеживать прогресс выполнения долгих операций, а их у нас будет много."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "y4cZwOqW2zcB"
   },
   "outputs": [],
   "source": [
    "def good_update_interval(total_iters, num_desired_updates):\n",
    "    exact_interval = total_iters / num_desired_updates\n",
    "    order_of_mag = len(str(total_iters)) - 1\n",
    "    round_mag = order_of_mag - 1\n",
    "    update_interval = int(round(exact_interval, -round_mag))\n",
    "\n",
    "    if update_interval == 0:\n",
    "        update_interval = 1\n",
    "\n",
    "    return update_interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phz1XjVQaYds"
   },
   "source": [
    "Напишем функцию создающая \"умные\" батчи:\n",
    "1. токенизируем и обрежем наши комментарии по длине(максимальная длина для BERT 512 символов, мы возьмём 400)\n",
    "2. Отсортируем наши последовательности батчей по из длине по возрастанию\n",
    "3. Создадим батчи с размер в 16 сэмплов\n",
    "4. Применим padding для каждого батча\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yI1R1Sjw1xnK"
   },
   "outputs": [],
   "source": [
    "max_len = 400\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "xbPDH4gMVEsf"
   },
   "outputs": [],
   "source": [
    "def make_smart_batches(text_samples, labels, batch_size):\n",
    "    \n",
    "    print('Создадим Smart Batches из {:,} текстов с размером батча {:,}...\\n'.format(len(text_samples), batch_size))\n",
    "\n",
    "    # ==============================================\n",
    "    #   Токенизация и усечение последовательностей\n",
    "    # ==============================================\n",
    "\n",
    "    full_input_ids = []\n",
    "\n",
    "    print('Токенизируем {:,} текстов...'.format(len(text_samples)))\n",
    "\n",
    "    # выберем интервал для вывода прогресса выполнения.\n",
    "    update_interval = good_update_interval(total_iters=len(text_samples), num_desired_updates=10)\n",
    "\n",
    "    # токениризируем каждый текст в переданнос датасете\n",
    "    for text in text_samples:\n",
    "        \n",
    "        # Вывод отчёта о прогрессе.\n",
    "        if ((len(full_input_ids) % update_interval) == 0):\n",
    "            print('  Токенизировано {:,} текстов.'.format(len(full_input_ids)))\n",
    "\n",
    "        # токениризуем сэмпл\n",
    "        input_ids = tokenizer.encode(text=text,              # текст.\n",
    "                                    add_special_tokens=True, # добавим символы начала и конца строки\n",
    "                                    max_length=max_len,      # ограничим наш текст 400 количеством символов\n",
    "                                    truncation=True,         # включим ограничение\n",
    "                                    padding=False)           # без применения паддинга\n",
    "                                    \n",
    "        full_input_ids.append(input_ids)\n",
    "        \n",
    "    print('DONE.')\n",
    "    print('{:>10,} текстов\\n'.format(len(full_input_ids)))\n",
    "\n",
    "    # =========================\n",
    "    #    определение батчей\n",
    "    # =========================    \n",
    "\n",
    "    # перед определением батчей, отсортируем на последовательности по длине по возрастанию\n",
    "    samples = sorted(zip(full_input_ids, labels), key=lambda x: len(x[0]))\n",
    "\n",
    "    print('{:>10,} текстов после сортировки\\n'.format(len(samples)))\n",
    "\n",
    "    \n",
    "\n",
    "    # списки для хранения полученных батчей\n",
    "    batch_ordered_sentences = []\n",
    "    batch_ordered_labels = []\n",
    "\n",
    "    print('Выборка батчей размера {:}...'.format(batch_size))\n",
    "\n",
    "    # выберем интервал для вывода прогресса выполнения.\n",
    "    update_interval = good_update_interval(total_iters=len(samples), num_desired_updates=10)\n",
    "    \n",
    "    while len(samples) > 0:\n",
    "        \n",
    "        # отчёт о прогрессе\n",
    "        if ((len(batch_ordered_sentences) % update_interval) == 0 \\\n",
    "            and not len(batch_ordered_sentences) == 0):\n",
    "            print('  Выборка батча {:,}.'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "        # сохраняем размер нашего батча, если это последний батч, то сохраняем остаток\n",
    "        # который по размеру меньше батча\n",
    "        to_take = min(batch_size, len(samples))\n",
    "\n",
    "        # сгенерируем рандомный индекс с которого будет начинаться наш батч\n",
    "        select = random.randint(0, len(samples) - to_take)\n",
    "\n",
    "        # выберем наш батч\n",
    "        batch = samples[select:(select + to_take)]\n",
    "\n",
    "        # так как в сэмпле храниться и класс и токениризоравнный текст с помощью кортежа, \n",
    "        # то в один список мы записываем нашу последовательность, а во второй класс\n",
    "        batch_ordered_sentences.append([s[0] for s in batch])\n",
    "        batch_ordered_labels.append([s[1] for s in batch])\n",
    "\n",
    "        # удаляем наш батч из выборки\n",
    "        del samples[select:select + to_take]\n",
    "\n",
    "    print('\\n  DONE - {:,} батчей выбрано.\\n'.format(len(batch_ordered_sentences)))\n",
    "\n",
    "    # =========================\n",
    "    #     Добавляем Padding\n",
    "    # =========================    \n",
    "\n",
    "    print('Padding последовательностей в каждом батче...')\n",
    "\n",
    "    py_inputs = []\n",
    "    py_attn_masks = []\n",
    "    py_labels = []\n",
    "\n",
    "    for (batch_inputs, batch_labels) in zip(batch_ordered_sentences, batch_ordered_labels):\n",
    "\n",
    "        batch_padded_inputs = []\n",
    "        batch_attn_masks = []\n",
    "        \n",
    "        # найдём размер саммой длинной последовательности в батче\n",
    "        max_size = max([len(sen) for sen in batch_inputs])\n",
    "\n",
    "        for sen in batch_inputs:\n",
    "            \n",
    "            # вычисляем количество токенов, которое на добавить\n",
    "            num_pads = max_size - len(sen)\n",
    "\n",
    "            # добавляем токены в последовательность\n",
    "            padded_input = sen + [tokenizer.pad_token_id]*num_pads\n",
    "\n",
    "            # добавим маску для выделения важных токенов\n",
    "            attn_mask = [1] * len(sen) + [0] * num_pads\n",
    "\n",
    "            # добавим результаты в списки\n",
    "            batch_padded_inputs.append(padded_input)\n",
    "            batch_attn_masks.append(attn_mask)\n",
    "\n",
    "        # сохраним наши батчи и преобразуем их в тензоры PyTorch\n",
    "        py_inputs.append(torch.tensor(batch_padded_inputs))\n",
    "        py_attn_masks.append(torch.tensor(batch_attn_masks))\n",
    "        py_labels.append(torch.tensor(batch_labels))\n",
    "    \n",
    "    print('  DONE.')\n",
    "\n",
    "    # Возвращаем наши Smart batches\n",
    "    return (py_inputs, py_attn_masks, py_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "Q-GhMS9BWPDR",
    "outputId": "a9908295-e027-4203-b924-c0a1e78a23a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создадим Smart Batches из 127,656 текстов с размером батча 16...\n",
      "\n",
      "Токенизируем 127,656 текстов...\n",
      "  Токенизировано 0 текстов.\n",
      "  Токенизировано 10,000 текстов.\n",
      "  Токенизировано 20,000 текстов.\n",
      "  Токенизировано 30,000 текстов.\n",
      "  Токенизировано 40,000 текстов.\n",
      "  Токенизировано 50,000 текстов.\n",
      "  Токенизировано 60,000 текстов.\n",
      "  Токенизировано 70,000 текстов.\n",
      "  Токенизировано 80,000 текстов.\n",
      "  Токенизировано 90,000 текстов.\n",
      "  Токенизировано 100,000 текстов.\n",
      "  Токенизировано 110,000 текстов.\n",
      "  Токенизировано 120,000 текстов.\n",
      "DONE.\n",
      "   127,656 текстов\n",
      "\n",
      "   127,656 текстов после сортировки\n",
      "\n",
      "Выборка батчей размера 16...\n",
      "\n",
      "  DONE - 7,979 батчей выбрано.\n",
      "\n",
      "Padding последовательностей в каждом батче...\n",
      "  DONE.\n"
     ]
    }
   ],
   "source": [
    "(train_py_inputs, train_py_attn_masks, train_py_labels) = make_smart_batches(train_text, train_labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Baz3kWcIyN00"
   },
   "source": [
    "Получим конфиг для нашей претринерованной модели DistilBert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-gTFKyhU8rO2",
    "outputId": "0ad7820e-2baa-4216-f8d1-caf758c9d6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config type: <class 'transformers.configuration_distilbert.DistilBertConfig'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(pretrained_model_name_or_path='distilbert-base-uncased',\n",
    "                                    num_labels=2)\n",
    "\n",
    "print('Config type:', str(type(config)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAaUpG-b0epy"
   },
   "source": [
    "Загрузим претринерованную модель DistilBert для классфикации, с пробросом конфига."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "5guhQIjE8q9P",
    "outputId": "19ca03ac-859e-4b0c-82bc-17aada5974d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model type: <class 'transformers.modeling_distilbert.DistilBertForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path='distilbert-base-uncased',\n",
    "    config=config)\n",
    "\n",
    "print('\\nModel type:', str(type(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXxIeFz-yw4r"
   },
   "source": [
    "Укажем модели использовать GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "cqzZTF_08qzq",
    "outputId": "208b2180-97ae-490c-b686-ab20f12a774b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model to GPU...\n",
      "  GPU: Tesla T4\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('\\nLoading model to GPU...')\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "print('  GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "desc = model.to(device)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz2EbTCiy-R_"
   },
   "source": [
    "Создадим оптимизаторы для нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SH2OxNnU8qlk"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 5e-5, \n",
    "                  eps = 1e-8 \n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "GQOvZ0MU8qXo"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 1\n",
    "total_steps = len(train_py_inputs) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3IRv_q1zXf6"
   },
   "source": [
    "Добавим функцию форматирования даты для вывода оставшегося время работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "AStamJz6Ev9p"
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6HYsVng0Dq9"
   },
   "source": [
    "Натренируем нашу модель\n",
    "\n",
    "P.S. код тренировки модели взять из библиотеки transformers\n",
    "https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "UvCwbDpB8qPH",
    "outputId": "366bc69c-a1ed-4efe-aa1c-30893a00cd1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Эпоха 1 / 1 ========\n",
      "Тренировка на 7,979 батчей...\n",
      "  Батч     800  из    7,979.    Прошло: 0:02:04.  Осталось: 0:18:30\n",
      "  Батч   1,600  из    7,979.    Прошло: 0:04:04.  Осталось: 0:16:11\n",
      "  Батч   2,400  из    7,979.    Прошло: 0:06:10.  Осталось: 0:14:19\n",
      "  Батч   3,200  из    7,979.    Прошло: 0:08:11.  Осталось: 0:12:13\n",
      "  Батч   4,000  из    7,979.    Прошло: 0:10:08.  Осталось: 0:10:05\n",
      "  Батч   4,800  из    7,979.    Прошло: 0:12:06.  Осталось: 0:08:01\n",
      "  Батч   5,600  из    7,979.    Прошло: 0:14:06.  Осталось: 0:05:59\n",
      "  Батч   6,400  из    7,979.    Прошло: 0:16:02.  Осталось: 0:03:57\n",
      "  Батч   7,200  из    7,979.    Прошло: 0:18:01.  Осталось: 0:01:57\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Тренировка эпохи заняла: 0:20:02\n",
      "\n",
      "Тренировка окончена!\n",
      "Тренировка заняла 0:20:02 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 321\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Будем сохранять потери, accuracy и время выполнения\n",
    "training_stats = []\n",
    "\n",
    "# выберем интервал для вывода прогресса выполнения.\n",
    "update_interval = good_update_interval(total_iters=len(train_py_inputs), num_desired_updates=10)\n",
    "\n",
    "# замеряем общее время выполнения\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Тренировка\n",
    "    # ========================================\n",
    "    \n",
    "    # Проходимся по всей тренировачной выборке за эпоху.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Эпоха {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    \n",
    "    # перед каждой эпохой снова рандомизируем нашу выборку\n",
    "    if epoch_i > 0:\n",
    "        (train_py_inputs, train_py_attn_masks, train_py_labels) = make_smart_batches(train_texts, train_labels, batch_size)\n",
    "    \n",
    "    print('Тренировка на {:,} батчей...'.format(len(train_py_inputs)))\n",
    "\n",
    "    # замеряем как долго тренировалась одна эпоха\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # установим модель в режим тренировки\n",
    "    model.train()\n",
    "    for step in range(0, len(train_py_inputs)):\n",
    "\n",
    "        # Вывод прогресса\n",
    "        if step % update_interval == 0 and not step == 0:\n",
    "            # расчитаем прошедшее время\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # расчитаем оставшееся время.\n",
    "            steps_per_sec = (time.time() - t0) / step\n",
    "            remaining_sec = steps_per_sec * (len(train_py_inputs) - step)\n",
    "            remaining = format_time(remaining_sec)\n",
    "\n",
    "            # вывод прогресса.\n",
    "            print('  Батч {:>7,}  из  {:>7,}.    Прошло: {:}.  Осталось: {:}'.format(step, len(train_py_inputs), elapsed, remaining))\n",
    "\n",
    "        # перенсём наши батчи на GPU\n",
    "        b_input_ids = train_py_inputs[step].to(device)\n",
    "        b_input_mask = train_py_attn_masks[step].to(device)\n",
    "        b_labels = train_py_labels[step].to(device)\n",
    "\n",
    "        # збрасываем градиенты\n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss, logits = model(b_input_ids, \n",
    "                             #token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # расчёт потери по всем батчам\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Вычисляем градиент\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Обновляем параметры оптимизатора\n",
    "        optimizer.step()\n",
    "\n",
    "        # Обновим в оптимизаторе скорость обучения.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Рассчитаем среднюю потерю по всем батчам\n",
    "    avg_train_loss = total_train_loss / len(train_py_inputs)            \n",
    "    \n",
    "    # измеряем затраченное время на эпоху\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Тренировка эпохи заняла: {:}\".format(training_time))\n",
    "        \n",
    "    # Записываем всю статистику тренировки.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Тренировка окончена!\")\n",
    "\n",
    "print(\"Тренировка заняла {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnwZLD2F33L6"
   },
   "source": [
    "Создадим smart батчи для тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "fxpYtcfU8qAj",
    "outputId": "2e0bde6a-eab3-4002-9672-8d726867e627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создадим Smart Batches из 31,915 текстов с размером батча 16...\n",
      "\n",
      "Токенизируем 31,915 текстов...\n",
      "  Токенизировано 0 текстов.\n",
      "  Токенизировано 3,000 текстов.\n",
      "  Токенизировано 6,000 текстов.\n",
      "  Токенизировано 9,000 текстов.\n",
      "  Токенизировано 12,000 текстов.\n",
      "  Токенизировано 15,000 текстов.\n",
      "  Токенизировано 18,000 текстов.\n",
      "  Токенизировано 21,000 текстов.\n",
      "  Токенизировано 24,000 текстов.\n",
      "  Токенизировано 27,000 текстов.\n",
      "  Токенизировано 30,000 текстов.\n",
      "DONE.\n",
      "    31,915 текстов\n",
      "\n",
      "    31,915 текстов после сортировки\n",
      "\n",
      "Выборка батчей размера 16...\n",
      "\n",
      "  DONE - 1,995 батчей выбрано.\n",
      "\n",
      "Padding последовательностей в каждом батче...\n",
      "  DONE.\n"
     ]
    }
   ],
   "source": [
    "(test_py_inputs, test_py_attn_masks, test_py_labels) = make_smart_batches(test_text, test_labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y64O6W6437-Z"
   },
   "source": [
    "Сделаем предсказания на тестовых батчах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "WvKJO2Wy8px1",
    "outputId": "d7d12735-6548-45c7-cf06-1b3561e700d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предскажем классы для 31,915 тестовых сэмплов...\n",
      "  Батч     200  из    1,995.    Прошло: 0:00:10.  Осталось: 0:01:33\n",
      "  Батч     400  из    1,995.    Прошло: 0:00:19.  Осталось: 0:01:15\n",
      "  Батч     600  из    1,995.    Прошло: 0:00:28.  Осталось: 0:01:04\n",
      "  Батч     800  из    1,995.    Прошло: 0:00:38.  Осталось: 0:00:57\n",
      "  Батч   1,000  из    1,995.    Прошло: 0:00:48.  Осталось: 0:00:48\n",
      "  Батч   1,200  из    1,995.    Прошло: 0:00:56.  Осталось: 0:00:37\n",
      "  Батч   1,400  из    1,995.    Прошло: 0:01:04.  Осталось: 0:00:27\n",
      "  Батч   1,600  из    1,995.    Прошло: 0:01:14.  Осталось: 0:00:18\n",
      "  Батч   1,800  из    1,995.    Прошло: 0:01:24.  Осталось: 0:00:09\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Предскажем классы для {:,} тестовых сэмплов...'.format(len(test_labels)))\n",
    "\n",
    "# Переведём модель в другой режим\n",
    "model.eval()\n",
    "\n",
    "predictions , true_labels = [], []\n",
    "\n",
    "update_interval = good_update_interval(total_iters=len(test_py_inputs), num_desired_updates=10)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "for step in range(0, len(test_py_inputs)):\n",
    "\n",
    "    if step % update_interval == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        \n",
    "        steps_per_sec = (time.time() - t0) / step\n",
    "        remaining_sec = steps_per_sec * (len(test_py_inputs) - step)\n",
    "        remaining = format_time(remaining_sec)\n",
    "\n",
    "        print('  Батч {:>7,}  из  {:>7,}.    Прошло: {:}.  Осталось: {:}'.format(step, len(test_py_inputs), elapsed, remaining))\n",
    "\n",
    "    b_input_ids = test_py_inputs[step].to(device)\n",
    "    b_input_mask = test_py_attn_masks[step].to(device)\n",
    "    b_labels = test_py_labels[step].to(device)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        # вычисляем предсказания\n",
    "        outputs = model(b_input_ids, \n",
    "                        #token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # переносим предсказания на CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "    # сохраняем предсказания\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "X8GYH12npSAr"
   },
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Choose the label with the highest score as our prediction.\n",
    "preds = np.argmax(predictions, axis=1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "INVt9wI-BRJY",
    "outputId": "0a340f55-e2f2-4bf0-e82f-6387f5ae760e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4QM2xOBk46Su",
    "outputId": "d9ab6f87-5a6b-48e0-8ab1-276c026f5a99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "3XWX_RrXW7Jj",
    "outputId": "c0c5624a-1d3d-4189-c7e9-b239035d065d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertClassification F1-score: 0.8385332904470891\n"
     ]
    }
   ],
   "source": [
    "print('DistilBertClassification F1-score: {}'.format(f1_score(true_labels, preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFZB2Hau8yvN"
   },
   "source": [
    "На DistillBertClassification нам удалось достигнуть F1-score 0.84 на тестовой выборке, это достаточно высокий показатель. И так как нам повезло с GPU в google colab модель натренировалась более чем быстро, если локально без GPU это время занимало чуть ли не 13 часов, то тут модель справилась за 10 минут, что не может не радовать, так же оптимизация батчей дала достаточно сильный прирост при тренировке и предсказании."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXd8jevmFJOE"
   },
   "source": [
    "**Вывод:** мы получили два разных подхода к предобработке текстов и предсказыванию классов, логистическая регрессия вместе с TF-IDF справились достаточно быстро даже на CPU, хоть и нужна была предварительная обработка данных, а вот Bert, хоть там и не требуется ручная предварительная обработка текстов, так быстро на CPU не запустишь, он будет работать в разы дольше, чем логистическая регрессия, иногда доходило до 13 часов, если не делать смарт батчи, отсортированные по длине последовательности, так что без GPU здесь не обойтись, но Bert оказался сильно точнее, чем логистическая регрессия, примерно на 10 пунктов. Так что при выборе надо учитывать множество факторов, если вы хотите запускать вашу модель на легковесном сервере без GPU, то вам подходит метод с логистической регрессией, если же вы хотите большую точность предсказаний и у вас есть возможность иметь сервер с хорошим GPU, то используейте BERT."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sentence_sentiment_prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
